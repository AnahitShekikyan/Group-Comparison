{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMfrJ7Dt9J9FgZ6zk8JIk6O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnahitShekikyan/ADS-509-Assignment-3.1-Group-Comparison/blob/main/ADS_509_Assignment_3_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Assignment 3.1: Group Comparison**\n",
        "\n",
        "#### **Course:** ADS 509, Applied Large Language Models for Data Science\n",
        "\n",
        "#### **Name:** Anna Shekikyan\n",
        "\n",
        "#### **Date:** 09/22/2025\n",
        "\n",
        "#### **GitHub:** https://github.com/AnahitShekikyan/ADS-509-Assignment-3.1-Group-Comparison\n",
        "\n",
        "#### **ipynb:** https://colab.research.google.com/drive/1EIudyZtoDizfVaCL1ruPOl-MwD3cgcnN?usp=sharing"
      ],
      "metadata": {
        "id": "lS77RG7eHEd-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-eESNydTPbHh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "! pip install emoji\n",
        "import emoji\n",
        "import pandas as pd, csv\n",
        "\n",
        "from collections import Counter, defaultdict\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "! pip install WordCloud\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# additional import\n",
        "import nltk\n",
        "import numpy as np\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from nltk.probability import FreqDist"
      ],
      "metadata": {
        "id": "UBiRdPFfPiPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Place any addtional functions or constants you need here.\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Some punctuation variations\n",
        "punctuation = set(punctuation) # speeds up comparison\n",
        "tw_punct = punctuation - {\"#\"}\n",
        "\n",
        "# Stopwords\n",
        "sw = stopwords.words(\"english\")\n",
        "\n",
        "# Two useful regex\n",
        "whitespace_pattern = re.compile(r\"\\s+\")\n",
        "hashtag_pattern = re.compile(r\"^#[0-9a-zA-Z]+\")\n",
        "\n",
        "# It's handy to have a full set of emojis\n",
        "all_language_emojis = set()\n",
        "\n",
        "for country in emoji.EMOJI_DATA :\n",
        "    for em in emoji.EMOJI_DATA[country] :\n",
        "        all_language_emojis.add(em)\n",
        "\n",
        "# and now our functions\n",
        "def descriptive_stats(tokens, num_tokens = 5, verbose=True) :\n",
        "    \"\"\"\n",
        "        Given a list of tokens, print number of tokens, number of unique tokens,\n",
        "        number of characters, lexical diversity, and num_tokens most common\n",
        "        tokens. Return a list of\n",
        "    \"\"\"\n",
        "\n",
        "     # Place your Module 2 solution here\n",
        "    num_tokens_total = len(tokens)\n",
        "    num_unique_tokens = len(set(tokens))\n",
        "    lexical_diversity = (num_unique_tokens / num_tokens_total) if num_tokens_total > 0 else 0.0\n",
        "    num_characters = sum(len(t) for t in tokens)\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"There are {num_tokens_total} tokens in the data.\")\n",
        "        print(f\"There are {num_unique_tokens} unique tokens in the data.\")\n",
        "        print(f\"There are {num_characters} characters in the data.\")\n",
        "        print(f\"The lexical diversity is {lexical_diversity:.3f} in the data.\")\n",
        "        # print the most common tokens\n",
        "        top = Counter(tokens).most_common(num_tokens)\n",
        "        print(f\"Top {num_tokens} tokens: {top}\")\n",
        "\n",
        "    return(0)\n",
        "\n",
        "\n",
        "def contains_emoji(s):\n",
        "\n",
        "    s = str(s)\n",
        "    emojis = [ch for ch in s if emoji.is_emoji(ch)]\n",
        "\n",
        "    return(len(emojis) > 0)\n",
        "\n",
        "\n",
        "def remove_stop(tokens) :\n",
        "    # modify this function to remove stopwords\n",
        "    sw_set = set(sw)\n",
        "    cleaned = []\n",
        "    for tok in tokens:\n",
        "        # keep hashtags and single-character emojis\n",
        "        if tok.startswith(\"#\"):\n",
        "            cleaned.append(tok)\n",
        "        elif len(tok) == 1 and tok in all_language_emojis:\n",
        "            cleaned.append(tok)\n",
        "        elif tok not in sw_set:\n",
        "            cleaned.append(tok)\n",
        "    return cleaned\n",
        "\n",
        "\n",
        "    #return(tokens)\n",
        "\n",
        "def remove_punctuation(text, punct_set=tw_punct) :\n",
        "    return(\"\".join([ch for ch in text if ch not in punct_set]))\n",
        "\n",
        "def tokenize(text) :\n",
        "    \"\"\" Splitting on whitespace rather than the book's tokenize function. That\n",
        "        function will drop tokens like '#hashtag' or '2A', which we need for Twitter. \"\"\"\n",
        "\n",
        "    # modify function to return tokens\n",
        "    s = str(text).lower()\n",
        "    # drop URLs and @mentions\n",
        "    s = re.sub(r\"https?://\\S+\", \" \", s)\n",
        "    s = re.sub(r\"@\\w+\", \" \", s)\n",
        "    # put spaces around emoji so they survive split\n",
        "    s = \"\".join((f\" {ch} \" if ch in all_language_emojis else ch) for ch in s)\n",
        "    # remove punctuation except '#'\n",
        "    s = remove_punctuation(s, punct_set=tw_punct)\n",
        "    # collapse whitespace and split to tokens\n",
        "    tokens = whitespace_pattern.sub(\" \", s).strip().split()\n",
        "\n",
        "    text = tokens\n",
        "    return(text)\n",
        "\n",
        "def prepare(text, pipeline) :\n",
        "    tokens = str(text)\n",
        "\n",
        "    for transform in pipeline :\n",
        "        tokens = transform(tokens)\n",
        "\n",
        "    return(tokens)"
      ],
      "metadata": {
        "id": "X96HDncQPuOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Ingestion\n",
        "\n",
        "Use this section to ingest your data into the data structures you plan to use. Typically this will be a dictionary or a pandas DataFrame."
      ],
      "metadata": {
        "id": "_609nOGlkw7e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data input, setup\n",
        "from google.colab import drive\n",
        "from pathlib import Path\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# the folder that directly contains both 'lyrics' and 'twitter'\n",
        "data_base = Path(\"/content/drive/MyDrive/M1 Assignment Data/M1 Assignment Data/M1 Results\").resolve()\n",
        "\n",
        "print(\"Using data base:\", data_base)\n",
        "print(\"lyrics exists:\", (data_base / \"lyrics\").is_dir(), \"| twitter exists:\", (data_base / \"twitter\").is_dir())"
      ],
      "metadata": {
        "id": "YCJRQq4XlP_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_location = \"/content/drive/MyDrive/M1 Assignment Data/M1 Assignment Data/M1 Results/\"\n",
        "twitter_folder = \"twitter/\"\n",
        "lyrics_folder = \"lyrics/\"\n",
        "\n",
        "artist_files = {'cher':'cher_followers_data.txt',\n",
        "                'robyn':'robynkonichiwa_followers_data.txt'}"
      ],
      "metadata": {
        "id": "7b1kcNTCPyM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "twitter_data =  pd.read_csv(data_location + twitter_folder + artist_files['cher'],\n",
        "                           sep=\"\\t\",\n",
        "                           quoting=3)\n",
        "\n",
        "twitter_data['artist'] = \"cher\""
      ],
      "metadata": {
        "id": "mKmUMY8OtV23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "twitter_data.head(2)"
      ],
      "metadata": {
        "id": "FH4oo2oekioz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "twitter_data_2 = pd.read_csv(data_location + twitter_folder + artist_files['robyn'],\n",
        "                             sep=\"\\t\",\n",
        "                             quoting=3)\n",
        "twitter_data_2['artist'] = \"robyn\"\n",
        "\n",
        "twitter_data = pd.concat([\n",
        "    twitter_data,twitter_data_2])\n",
        "\n",
        "del(twitter_data_2)"
      ],
      "metadata": {
        "id": "E0oFzGU4AoZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read in the lyrics here\n",
        "# data_location and lyrics_folder are already set above\n",
        "base_lyrics_dir = data_location + lyrics_folder\n",
        "\n",
        "def load_txt_folder(artist: str) -> pd.DataFrame:\n",
        "    folder = os.path.join(base_lyrics_dir, artist)\n",
        "    rows = []\n",
        "    for fname in os.listdir(folder):\n",
        "        if fname.lower().endswith(\".txt\"):\n",
        "            path = os.path.join(folder, fname)\n",
        "            with open(path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
        "                text = f.read()\n",
        "            # simple title from filename\n",
        "            title = fname[:-4]  # drop .txt\n",
        "            if title.lower().startswith(artist.lower() + \"_\"):\n",
        "                title = title[len(artist) + 1:]\n",
        "            rows.append({\n",
        "                \"lyrics\": text,\n",
        "                \"title\": title,\n",
        "                \"artist\": artist,\n",
        "                \"source_file\": fname\n",
        "            })\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "lyrics_cher  = load_txt_folder(\"cher\")\n",
        "lyrics_robyn = load_txt_folder(\"robyn\")\n",
        "\n",
        "lyrics_data = pd.concat([lyrics_cher, lyrics_robyn], ignore_index=True)\n",
        "\n",
        "# column fro pipeline uses\n",
        "LYR_COL = \"lyrics\"\n",
        "\n",
        "print(\"Loaded lyrics_data:\", lyrics_data.shape)\n",
        "lyrics_data.head(3)"
      ],
      "metadata": {
        "id": "nB6G2a40GSsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization and Normalization\n",
        "\n",
        "In this next section, tokenize and normalize your data. We recommend the following cleaning.\n",
        "\n",
        "**Lyrics**\n",
        "\n",
        "* Remove song titles\n",
        "* Casefold to lowercase\n",
        "* Remove stopwords (optional)\n",
        "* Remove punctuation\n",
        "* Split on whitespace\n",
        "\n",
        "Removal of stopwords is up to you. Your descriptive statistic comparison will be different if you include stopwords, though TF-IDF should still find interesting features for you. Note that we remove stopwords before removing punctuation because the stopword set includes punctuation.\n",
        "\n",
        "**Twitter Descriptions**\n",
        "\n",
        "* Casefold to lowercase\n",
        "* Remove stopwords\n",
        "* Remove punctuation other than emojis or hashtags\n",
        "* Split on whitespace\n",
        "\n",
        "Removing stopwords seems sensible for the Twitter description data. Remember to leave in emojis and hashtags, since you analyze those."
      ],
      "metadata": {
        "id": "OGnEObaJIbLZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize + normalize lyrics\n",
        "lyrics_pipeline = [str.lower, remove_punctuation, tokenize, remove_stop]  # remove_stop optional\n",
        "\n",
        "lyrics_data = lyrics_data.copy()\n",
        "lyrics_data[\"tokens\"] = lyrics_data[\"lyrics\"].apply(prepare, pipeline=lyrics_pipeline)\n",
        "lyrics_data[\"num_tokens\"] = lyrics_data[\"tokens\"].map(len)\n",
        "\n",
        "print(lyrics_data.groupby(\"artist\")[\"title\"].count())\n",
        "lyrics_data.head(3)[[\"artist\",\"title\",\"num_tokens\"]]"
      ],
      "metadata": {
        "id": "gAESqhRVH9k_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drop missing bios first\n",
        "twitter_data = twitter_data.dropna(subset=[\"description\"]).reset_index(drop=True)\n",
        "\n",
        "# tokenize + normalize twitter (keeps hashtags/emojis by design of tokenize)\n",
        "twitter_pipeline = [str.lower, tokenize, remove_stop]\n",
        "\n",
        "twitter_data = twitter_data.copy()\n",
        "twitter_data[\"tokens\"] = twitter_data[\"description\"].apply(prepare, pipeline=twitter_pipeline)\n",
        "twitter_data[\"num_tokens\"] = twitter_data[\"tokens\"].map(len)\n",
        "\n",
        "print(twitter_data.groupby(\"artist\")[\"screen_name\"].count())\n",
        "twitter_data.head(3)[[\"artist\",\"description\",\"tokens\"]]"
      ],
      "metadata": {
        "id": "ElOX2M8-k50_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "twitter_data['has_emoji'] = twitter_data[\"description\"].apply(contains_emoji)"
      ],
      "metadata": {
        "id": "ph0k07JLJjdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a quick look at some descriptions with emojis."
      ],
      "metadata": {
        "id": "hcOiJyijKS1m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "twitter_data[twitter_data.has_emoji].sample(10)[[\"artist\",\"description\",\"tokens\"]]"
      ],
      "metadata": {
        "id": "YZPxvAB9KHEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the data processed, we can now start work on the assignment questions.\n",
        "\n",
        "`Q:` What is one area of improvement to your tokenization that you could theoretically carry out? (No need to actually do it; let's not make perfect the enemy of good enough.)\n",
        "\n",
        "`A:` **Areas for improvement:** handling mixed tokens (split #artistğŸ¤ â†’ #artist, ğŸ¤), standardizing hyphenation (â€œhype-girlâ€ â†’ hype, girl or keep as one), and optionally segmenting camelCase hashtags (#WeAllSleepAlone â†’ we, all, sleep, alone)."
      ],
      "metadata": {
        "id": "whxe90koKuYr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculate descriptive statistics on the two sets of lyrics and compare the results."
      ],
      "metadata": {
        "id": "4fEoW68iMcWa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pipeline for lyrics\n",
        "lyrics_pipeline = [str.lower, remove_punctuation, tokenize, remove_stop]\n",
        "\n",
        "# split by artist\n",
        "cher_lyrics  = lyrics_data.loc[lyrics_data[\"artist\"]==\"cher\",  LYR_COL].dropna()\n",
        "robyn_lyrics = lyrics_data.loc[lyrics_data[\"artist\"]==\"robyn\", LYR_COL].dropna()\n",
        "\n",
        "# tokenize per song (doc)\n",
        "cher_docs  = [prepare(t, lyrics_pipeline) for t in cher_lyrics]\n",
        "robyn_docs = [prepare(t, lyrics_pipeline) for t in robyn_lyrics]\n",
        "\n",
        "# flatten to corpus level\n",
        "cher_all  = [tok for doc in cher_docs  for tok in doc]\n",
        "robyn_all = [tok for doc in robyn_docs for tok in doc]\n",
        "\n",
        "print(\"cher â€” lyrics\")\n",
        "descriptive_stats(cher_all, num_tokens=15, verbose=True)\n",
        "\n",
        "print(\"\\nrobyn â€” lyrics\")\n",
        "descriptive_stats(robyn_all, num_tokens=15, verbose=True)\n",
        "\n",
        "# compact comparison table\n",
        "import numpy as np, pandas as pd\n",
        "def summarize(docs):\n",
        "    toks = [tok for d in docs for tok in d]\n",
        "    types = set(toks)\n",
        "    lens = [len(d) for d in docs]\n",
        "    return pd.Series({\n",
        "        \"songs\": len(docs),\n",
        "        \"total_tokens\": len(toks),\n",
        "        \"unique_types\": len(types),\n",
        "        \"TTR\": (len(types) / (len(toks) or 1)),\n",
        "        \"tokens_per_song_mean\": float(np.mean(lens) if lens else 0.0),\n",
        "        \"tokens_per_song_median\": float(np.median(lens) if lens else 0.0),\n",
        "    })\n",
        "\n",
        "comparison = pd.concat(\n",
        "    {\"cher\": summarize(cher_docs), \"robyn\": summarize(robyn_docs)},\n",
        "    axis=1\n",
        ")\n",
        "comparison"
      ],
      "metadata": {
        "id": "AKWQUmikKfqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Q:` What observations do you make about these data?\n",
        "\n",
        "`A:` Cherâ€™s corpus is larger (316 songs, 35.9k tokens) but shows lower lexical diversity (TTRâ‰ˆ0.103), likely due to repetition across many tracks. Robynâ€™s smaller set (104 songs, 15.2k tokens) has higher diversity (TTRâ‰ˆ0.142) and longer songs on average (â‰ˆ146 vs. â‰ˆ114 tokens). Top words suggest Cher leans toward romance/ballad vocabulary (â€œlove,â€ â€œheartâ€), while Robyn mixes pop romance with dance/club language (â€œdance,â€ â€œbeatâ€)."
      ],
      "metadata": {
        "id": "Q2yM1rDBMmYi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Find tokens uniquely related to a corpus\n",
        "\n",
        "Typically we would use TF-IDF to find unique tokens in documents. Unfortunately, we either have too few documents (if we view each data source as a single document) or too many (if we view each description as a separate document). In the latter case, our problem will be that descriptions tend to be short, so our matrix would be too sparse to support analysis.\n",
        "\n",
        "To avoid these problems, we will create a custom statistic to identify words that are uniquely related to each corpus. The idea is to find words that occur often in one corpus and infrequently in the other(s). Since corpora can be of different lengths, we will focus on the _concentration_ of tokens within a corpus. \"Concentration\" is simply the count of the token divided by the total corpus length. For instance, if a corpus had length 100,000 and a word appeared 1,000 times, then the concentration would be $\\frac{1000}{100000} = 0.01$. If the same token had a concentration of $0.005$ in another corpus, then the concentration ratio would be $\\frac{0.01}{0.005} = 2$. Very rare words can easily create infinite ratios, so you will also add a cutoff to your code so that a token must appear at least $n$ times for you to return it.\n",
        "\n",
        "An example of these calculations can be found in [this spreadsheet](https://docs.google.com/spreadsheets/d/1P87fkyslJhqXFnfYezNYrDrXp_GS8gwSATsZymv-9ms). Please don't hesitate to ask questions if this is confusing.\n",
        "\n",
        "In this section find 10 tokens for each of your four corpora that meet the following criteria:\n",
        "\n",
        "1. The token appears at least `n` times in all corpora\n",
        "1. The tokens are in the top 10 for the highest ratio of appearances in a given corpora vs appearances in other corpora.\n",
        "\n",
        "You will choose a cutoff for yourself based on the side of the corpus you're working with. If you're working with the Robyn-Cher corpora provided, `n=5` seems to perform reasonably well."
      ],
      "metadata": {
        "id": "CmqHXG9dPyJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find tokens uniquely related to a corpus via concentration ratios\n",
        "\n",
        "# flatten twitter tokens\n",
        "flat_cher_tw = [tok for tokens_list in twitter_data.loc[twitter_data[\"artist\"]==\"cher\", \"tokens\"].dropna() for tok in tokens_list]\n",
        "flat_robyn_tw = [tok for tokens_list in twitter_data.loc[twitter_data[\"artist\"]==\"robyn\", \"tokens\"].dropna() for tok in tokens_list]\n",
        "\n",
        "\n",
        "# build corpora\n",
        "corpora = {\n",
        "    \"cher_lyrics\":  cher_all,\n",
        "    \"robyn_lyrics\": robyn_all,\n",
        "    \"cher_twitter\": flat_cher_tw,\n",
        "    \"robyn_twitter\": flat_robyn_tw,\n",
        "}\n",
        "\n",
        "# counters and totals\n",
        "counters = {k: Counter(v) for k, v in corpora.items()}\n",
        "totals   = {k: sum(cnt.values()) for k, cnt in counters.items()}\n",
        "\n",
        "# global cutoff n (appears at least n times in ALL corpora)\n",
        "n = 5  # adjust if needed\n",
        "common_vocab = set.intersection(*[\n",
        "    {tok for tok, c in cnt.items() if c >= n}\n",
        "    for cnt in counters.values()\n",
        "])\n",
        "\n",
        "# helper to compute top-k distinctive tokens for one corpus\n",
        "def top_distinctive_for(target_name, topk=10):\n",
        "    target_cnt = counters[target_name]\n",
        "    target_tot = totals[target_name]\n",
        "\n",
        "    # aggregate \"other corpora\"\n",
        "    other_names = [k for k in counters if k != target_name]\n",
        "    other_cnt_sum = Counter()\n",
        "    other_tot_sum = 0\n",
        "    for name in other_names:\n",
        "        other_cnt_sum.update(counters[name])\n",
        "        other_tot_sum += totals[name]\n",
        "\n",
        "    rows = []\n",
        "    for tok in common_vocab:\n",
        "        c_i = target_cnt.get(tok, 0)\n",
        "        c_o = other_cnt_sum.get(tok, 0)\n",
        "        conc_i = c_i / target_tot if target_tot else 0.0\n",
        "        conc_o = c_o / other_tot_sum if other_tot_sum else 0.0\n",
        "        if conc_o == 0:\n",
        "            # skip if totally absent elsewhere after cutoff\n",
        "            continue\n",
        "        ratio = conc_i / conc_o\n",
        "        rows.append((tok, c_i, conc_i, conc_o, ratio))\n",
        "\n",
        "    df = pd.DataFrame(rows, columns=[\"token\", \"count_in_corpus\", \"conc_in_corpus\", \"conc_in_others\", \"ratio\"])\n",
        "    df = df.sort_values(\"ratio\", ascending=False).head(topk).reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "# top-10 for each corpus\n",
        "print(\"cher â€” Lyrics\"); display(top_distinctive_for(\"cher_lyrics\",  topk=10))\n",
        "print(\"robyn â€” Lyrics\"); display(top_distinctive_for(\"robyn_lyrics\", topk=10))\n",
        "print(\"cher â€” Twitter\"); display(top_distinctive_for(\"cher_twitter\", topk=10))\n",
        "print(\"robyn â€” Twitter\"); display(top_distinctive_for(\"robyn_twitter\", topk=10))"
      ],
      "metadata": {
        "id": "DTNO_q4LrFGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Q:` What are some observations about the top tokens? Do you notice any interesting items on the list?\n",
        "\n",
        "`A:` Cherâ€™s lyric-unique tokens skew toward interjections and section markersâ€”â€œooh,â€ â€œohh,â€ â€œooo,â€ and especially â€œchorusâ€â€”plus emotive/action words like â€œtonight,â€ â€œtears,â€ â€œgonna,â€ and â€œgotta,â€ which fits a pop-ballad, verse/chorus style. Robynâ€™s lyric-unique list leans club/pop: â€œbeat,â€ â€œcrash,â€ â€œalright,â€ with â€œ88â€ pointing to a specific track/era and normalized contractions like â€œitll.â€\n",
        "\n",
        "On Twitter, Cherâ€™s followers emphasize identity/values (â€œgod,â€ â€œproud,â€ â€œfaith,â€ â€œfriend,â€ â€œwoman,â€ â€œtruth,â€ â€œmamaâ€), while Robynâ€™s emphasize music/nightlife vocabulary (â€œmusic,â€ â€œdance,â€ â€œsound,â€ â€œspinning,â€ â€œheadâ€) and timing variants (â€œtill/tilâ€). The frequent â€œchorusâ€ token likely comes from transcription headers and could be filtered if cleaner semantic signals are desired."
      ],
      "metadata": {
        "id": "fJIxBT72REYr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build word clouds for all four corpora.\n",
        "\n",
        "For building wordclouds, we'll follow exactly the code of the text. The code in this section can be found [here](https://github.com/blueprints-for-text-analytics-python/blueprints-text/blob/master/ch01/First_Insights.ipynb). If you haven't already, you should absolutely clone the repository that accompanies the book."
      ],
      "metadata": {
        "id": "KyFvKT9mSOSR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def wordcloud(word_freq, title=None, max_words=200, stopwords=None):\n",
        "\n",
        "    wc = WordCloud(width=800, height=400,\n",
        "                   background_color= \"black\", colormap=\"Paired\",\n",
        "                   max_font_size=150, max_words=max_words)\n",
        "\n",
        "    # convert data frame into dict\n",
        "    if type(word_freq) == pd.Series:\n",
        "        counter = Counter(word_freq.fillna(0).to_dict())\n",
        "    else:\n",
        "        counter = word_freq\n",
        "\n",
        "    # filter stop words in frequency counter\n",
        "    if stopwords is not None:\n",
        "        counter = {token:freq for (token, freq) in counter.items()\n",
        "                              if token not in stopwords}\n",
        "    wc.generate_from_frequencies(counter)\n",
        "\n",
        "    plt.title(title)\n",
        "\n",
        "    plt.imshow(wc, interpolation='bilinear')\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "\n",
        "def count_words(df, column='tokens', preprocess=None, min_freq=2):\n",
        "\n",
        "    # process tokens and update counter\n",
        "    def update(doc):\n",
        "        tokens = doc if preprocess is None else preprocess(doc)\n",
        "        counter.update(tokens)\n",
        "\n",
        "    # create counter and run through all data\n",
        "    counter = Counter()\n",
        "    df[column].map(update)\n",
        "\n",
        "    # transform counter into data frame\n",
        "    freq_df = pd.DataFrame.from_dict(counter, orient='index', columns=['freq'])\n",
        "    freq_df = freq_df.query('freq >= @min_freq')\n",
        "    freq_df.index.name = 'token'\n",
        "\n",
        "    return freq_df.sort_values('freq', ascending=False)"
      ],
      "metadata": {
        "id": "qdZPQ-HIQctu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# word clouds for all four corpora, frequencies\n",
        "cher_lyrics_freq  = count_words(lyrics_data[lyrics_data[\"artist\"]==\"cher\"],  column=\"tokens\", min_freq=2)\n",
        "robyn_lyrics_freq = count_words(lyrics_data[lyrics_data[\"artist\"]==\"robyn\"], column=\"tokens\", min_freq=2)\n",
        "\n",
        "cher_tw_freq  = count_words(twitter_data[twitter_data[\"artist\"]==\"cher\"],  column=\"tokens\", min_freq=2)\n",
        "robyn_tw_freq = count_words(twitter_data[twitter_data[\"artist\"]==\"robyn\"], column=\"tokens\", min_freq=2)\n",
        "\n",
        "# word clouds\n",
        "plt.figure(figsize=(8,6)); wordcloud(cher_lyrics_freq[\"freq\"],  title=\"cher â€” lyrics\")\n",
        "plt.figure(figsize=(8,6)); wordcloud(robyn_lyrics_freq[\"freq\"], title=\"robyn â€” lyrics\")\n",
        "plt.figure(figsize=(8,6)); wordcloud(cher_tw_freq[\"freq\"],      title=\"cher â€” twitter\")\n",
        "plt.figure(figsize=(8,6)); wordcloud(robyn_tw_freq[\"freq\"],     title=\"robyn â€” twitter\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "qJI2q-8GSidG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Q:` What observations do you have about these (relatively straightforward) wordclouds?\n",
        "\n",
        "`A:` The lyric clouds are classic pop: Cher leans into romance/ballad words (â€œlove,â€ â€œheart,â€ â€œtime,â€ â€œbabyâ€), while Robyn mixes that with clear dance-club terms (â€œdance,â€ â€œbeat,â€ â€œboom,â€ â€œkillingâ€). The Twitter clouds center on bio staples (â€œmusic,â€ â€œlove,â€ â€œlife,â€ â€œfollowâ€) plus identity tags (â€œsheher,â€ â€œhehimâ€), with language hintsâ€”Spanish (â€œde,â€ â€œlaâ€) around Cher and Swedish (â€œoch,â€ â€œpÃ¥,â€ â€œÃ¤r,â€ â€œjagâ€) around Robyn. Normalization shows up as â€œim/dont/youre,â€ but overall the visuals match the earlier story: Cher = classic pop/romance; Robyn = pop with a club edge."
      ],
      "metadata": {
        "id": "2NwDqBY5UHKk"
      }
    }
  ]
}